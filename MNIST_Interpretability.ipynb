{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from InterpretabilityMethods import *\n",
    "from SequentialTask import *\n",
    "from EWCMethods import EWC_Method\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"models/mnist_intepretability_model\"\n",
    "TRAIN_MODEL = False\n",
    "LOAD_MODEL = True\n",
    "\n",
    "task_digit_labels = [0,1]\n",
    "epochs = 5\n",
    "training_batches = 300\n",
    "validation_batches = 100\n",
    "batch_size = 32\n",
    "ewc_method = EWC_Method.NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING MODEL\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 24, 24, 12)        312       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 12)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 9, 9, 12)          2316      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 4, 4, 12)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 1, 12)          2316      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                130       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,096\n",
      "Trainable params: 5,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model: tf.keras.Model\n",
    "if LOAD_MODEL and os.path.exists(MODEL_SAVE_PATH):\n",
    "    # Try to load model directly, if one exists\n",
    "    print(\"LOADING MODEL\")\n",
    "    model = tf.keras.models.load_model(MODEL_SAVE_PATH,compile=False)  # type: ignore\n",
    "else:\n",
    "    # Otherwise, make an entire new model!\n",
    "    print(\"CREATING MODEL\")\n",
    "    model_input_shape = (28,28,1)\n",
    "    model_inputs = model_layer = tf.keras.Input(shape=model_input_shape)\n",
    "    model_layer = tf.keras.layers.Conv2D(12, (5,5), activation=\"relu\")(model_layer)\n",
    "    model_layer = tf.keras.layers.MaxPool2D((2,2))(model_layer)\n",
    "    model_layer = tf.keras.layers.Conv2D(12, (4,4), activation=\"relu\")(model_layer)\n",
    "    model_layer = tf.keras.layers.MaxPool2D((2,2))(model_layer)\n",
    "    model_layer = tf.keras.layers.Conv2D(12, (4,4), activation=\"relu\")(model_layer)\n",
    "    model_layer = tf.keras.layers.Flatten()(model_layer)\n",
    "    model_layer = tf.keras.layers.Dense(10, activation=\"relu\")(model_layer)\n",
    "    model_layer = tf.keras.layers.Dense(len(task_digit_labels))(model_layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=model_inputs, outputs=model_layer, name=\"model\")\n",
    "\n",
    "if len(task_digit_labels) == 2:\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "else:\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task = MNISTClassificationTask(\n",
    "        name=f\"Task 0\",\n",
    "        model=model,\n",
    "        model_base_loss=loss_fn,\n",
    "        task_digit_labels=task_digit_labels,\n",
    "        training_batches = training_batches,\n",
    "        validation_batches = validation_batches,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "task.compile_model(loss_fn)\n",
    "\n",
    "# Show some images from the task dataset\n",
    "batch = task.training_dataset.take(1).get_single_element()\n",
    "images = batch[0][:9]\n",
    "plot_images(images, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    checkpoint_path = MODEL_SAVE_PATH+\"/checkpoint\"\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        verbose=1)\n",
    "\n",
    "    history = task.train_on_task(epochs=epochs, callbacks=[\n",
    "        # checkpoint_callback,\n",
    "    ])\n",
    "    model.save(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_inspection(model=task.model, layer_index=5, steps=1000, step_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occlusion_sensitivity(\n",
    "    model: tf.keras.Model,\n",
    "    data: tf.data.Dataset,\n",
    "    num_items: int,\n",
    "    patch_size: int = 3,\n",
    "    stride: int = 1):\n",
    "    \"\"\"\n",
    "    Compute sensitivity to a series of data points by occluding a section of the image\n",
    "    Occlusion is shifted across the image, with loss being computed at each step\n",
    "\n",
    "    Parameters:\n",
    "        model: tf.keras.Model\n",
    "            The model to test sensitivity of\n",
    "        data: tf.data.Dataset\n",
    "            The data to test over. Should be a subsection of the entire dataset\n",
    "            Use data.take() to get a small subset of data first\n",
    "            Each image is processed, so the size of this dataset is number of images\n",
    "        num_items: int\n",
    "            The number of items to process overall (from the dataset)\n",
    "        patch_size: int\n",
    "            The size of the patch use for occlusion\n",
    "        stride: int\n",
    "            The amount to move the patch between trials\n",
    "            Currently unused (stride = patch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    processed_images = 0\n",
    "\n",
    "    # Loop over each batch in the dataset\n",
    "    for batch in data:\n",
    "        # Pull out the image/label pair from tuple\n",
    "        # Each of these variables have dimension starting with batch_size\n",
    "        # So zip them together to get image, label pairs directly\n",
    "        images = batch[0]\n",
    "        labels = batch[1]\n",
    "        for image, label in zip(images, labels):\n",
    "            # Convert the image to something more... manageable\n",
    "            # Remove first index (added from tf dataset)\n",
    "            # convert to numpy array\n",
    "            image: np.ndarray = np.array(image)  # type: ignore\n",
    "            sensitivity_map = np.zeros((image.shape[0], image.shape[1]))\n",
    "            patched_images = []\n",
    "            # Loop over every possible position of the occulsion square\n",
    "            # note for now the stride is simply the patch size, i.e.\n",
    "            # disjoint patches\n",
    "            for top_left_y in range(0, image.shape[0],patch_size):\n",
    "                for top_left_x in range(0, image.shape[1],patch_size):\n",
    "                    # Copy the original image, apply a square of black over that patch\n",
    "                    patched_image = np.array(image, copy=True)\n",
    "                    patched_image[\n",
    "                        top_left_y:top_left_y+patch_size,\n",
    "                        top_left_x:top_left_x+patch_size, \n",
    "                        : \n",
    "                    ] = 0.0\n",
    "                    # We collect all the patched images together to be processed in a single batch\n",
    "                    patched_images.append(patched_image)\n",
    "            patched_images = np.array(patched_images)\n",
    "            # Note multiplying by label, we are only interested in \n",
    "            # model predicted likelihood of correct class\n",
    "            predictions = model(patched_images) * label\n",
    "            # Loop over predictions and apply confidence to the sensitivity map\n",
    "            prediction_index = 0\n",
    "            for top_left_y in range(0, image.shape[0],patch_size):\n",
    "                for top_left_x in range(0, image.shape[1],patch_size):\n",
    "                    confidence = predictions[prediction_index] \n",
    "                    sensitivity_map[\n",
    "                        top_left_y:top_left_y+patch_size,\n",
    "                        top_left_x:top_left_x+patch_size,\n",
    "                    ] = confidence\n",
    "            # Finally finished with one image! Check if we have met quota\n",
    "            processed_images += 1\n",
    "            if processed_images >= num_items:\n",
    "                break\n",
    "\n",
    "data = task.validation_dataset.take(1)\n",
    "occlusion_sensitivity(task.model,data,num_items=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24d1b1f2d3141c0ff1635b20953f5ae3bc501888575bad739f03d5f2516fa480"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
